# -*- coding: utf-8 -*-
"""Dry_Bean_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PqK3bjlfkER0hvtf6oddtOM0rHGVL-if

Additional Information of Dry Bean Dataset

Seven different types of dry beans were used in this research, taking into account the features such as form, shape, type, and structure by the market situation. A computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. Bean images obtained by computer vision system were subjected to segmentation and feature extraction stages, and a total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.


Dataset Source: [Dry Bean Dataset](https://archive.ics.uci.edu/dataset/602/dry+bean+dataset) by [UCI Machine Learning Repository](https://archive.ics.uci.edu/), licensed under CC BY 4.0.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

df = pd.read_csv("Dry_Bean_Dataset.csv")
df.head()

df.info()

class_distribution = df['Class'].value_counts()

plt.figure(figsize=(8, 6))
class_distribution.plot(kind='bar', color='skyblue')
plt.title('Sınıf Dağılımı')
plt.xlabel('Sınıflar')
plt.ylabel('Örnek Sayısı')
plt.show()

numerical_df = df.copy()

"""*Converting the entire dataset to numerical format is crucial for machine learning algorithms as they typically operate on numerical data. Numerical representations allow algorithms to process and analyze the features effectively. This transformation ensures compatibility with a wide range of machine learning models, facilitating seamless integration into the training and prediction pipelines. Moreover, numerical data enables the algorithms to capture meaningful patterns, relationships, and dependencies within the dataset, contributing to more accurate and robust model outcomes.*


"""

from sklearn.preprocessing import LabelEncoder

# LabelEncoder'ı oluştur
label_encoder = LabelEncoder()

numerical_df["Class"] = label_encoder.fit_transform(numerical_df['Class'])

numeric_columns = numerical_df.columns[:-1]
numerical_df[numeric_columns] = numerical_df[numeric_columns].astype(str).apply(lambda x: x.str.replace(',', '.')).astype(float)

numerical_df.head(20)

numerical_df.info()

"""# Principal Component Analysis (PCA):

What it is:
PCA is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining the most important information. It achieves this by identifying the principal components, which are linear combinations of the original features. These components are ordered by their ability to explain the variance in the data.

**What it does:**

*Dimensionality Reduction:* Reduces the number of features, retaining the most significant ones.
*Decorrelation:* Principal components are orthogonal, reducing multicollinearity.
*Data Compression:* Preserves the majority of the dataset's variance in a
lower-dimensional representation.

**Use Cases:**
*   Visualization: Project high-dimensional data into 2D or 3D for visualization.
*  Noise Reduction: Remove less significant features, emphasizing the main patterns.
*   Feature Engineering: Create new features capturing the most variance in the data.


# t-Distributed Stochastic Neighbor Embedding (t-SNE):

**What it is:**
t-SNE is a nonlinear dimensionality reduction technique specifically designed for visualization. It models the similarity between data points in high-dimensional space and represents them in a lower-dimensional space, often 2D or 3D. t-SNE preserves local relationships while emphasizing global structures.

**What it does:**

Preservation of Local Structures: Retains the neighborhood relationships between data points.
Nonlinearity: Captures nonlinear patterns and complex structures in the data.
Optimal Visualization: Particularly effective for visualizing clusters and patterns in the data.

**Use Cases:**

*   Clustering Analysis: Identifying natural groupings in the data.
*   Outlier Detection: Detecting anomalies or outliers in the dataset.
*   Pattern Recognition: Visualizing complex structures in high-dimensional data.

**Considerations:**


*   Interpretability: t-SNE emphasizes visualization over interpretability.

*   Computational Intensity: Computationally expensive for large datasets.

*In summary, PCA is a linear technique focusing on overall data variance and dimensionality reduction, while t-SNE is a nonlinear technique emphasizing local relationships for effective visualization, particularly in clustering scenarios. Each has its strengths and is chosen based on the specific goals of the analysis.*




"""

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler

X = numerical_df.iloc[:, :-1]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA ile Boyut Azaltma
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# T-SNE ile Boyut Azaltma
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Sonuçları Görselleştirme
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')
plt.title('PCA - Boyut Azaltma')

plt.subplot(1, 2, 2)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', edgecolor='k')
plt.title('T-SNE - Boyut Azaltma')

plt.show()

from sklearn.model_selection import train_test_split
X = numerical_df.iloc[:, :-1]
y = numerical_df["Class"]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)

"""# StandardScaler Usage
The *StandardScaler* is applied to standardize the features of a dataset in machine learning. This involves transforming the data to have a mean of 0 and a standard deviation of 1, ensuring consistent scales across features. Standardization is crucial for algorithms sensitive to feature scales, promoting better convergence and performance. In the provided code,
**scaler.fit_transform(X_train)** computes mean and standard deviation from the training set, standardizing it, while **scaler.transform(X_test)** uses these values to standardize the test set. This preprocessing step optimizes the performance of various machine learning models, fostering stable and accurate predictions.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# GridSearchCV:
**GridSearchCV** is a hyperparameter tuning technique widely used in machine learning. It systematically searches through a predefined hyperparameter space to find the optimal set of hyperparameters for a given model. This exhaustive search is performed using cross-validation to evaluate model performance across different parameter combinations. GridSearchCV helps in automating the process of hyperparameter tuning, ensuring that the model achieves the best performance without manual trial and error. By selecting the optimal hyperparameters, GridSearchCV enhances the generalization and predictive capabilities of machine learning models, making them more robust and effective.

# k-Nearest Neighbors (KNN):
**KNN** is a simple and versatile machine learning algorithm used for both classification and regression tasks. The algorithm classifies a data point based on the majority class of its k nearest neighbors in the feature space. In the context of classification, the output is a class label, while for regression, it predicts a continuous value. KNN relies on the assumption that similar instances in the feature space tend to belong to the same class or have similar output values. It is a non-parametric, lazy-learning algorithm, meaning it doesn't make assumptions about the underlying data distribution during training. KNN is easy to implement and understand, making it a popular choice for various applications
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


knn = KNeighborsClassifier()


param_grid_knn = {'n_neighbors': [3, 5, 7],
                  'weights': ['uniform', 'distance'],
                  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}


grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')
grid_search_knn.fit(X_train_scaled, y_train)


print("KNN - En iyi parametreler:", grid_search_knn.best_params_)
print("KNN - En iyi skor:", grid_search_knn.best_score_)


y_pred_knn = grid_search_knn.predict(X_test_scaled)
print("KNN - Sınıflandırma Raporu:\n", classification_report(y_test, y_pred_knn))

"""# Naive Bayes:
**Naive Bayes** is a probabilistic machine learning algorithm based on Bayes' theorem. It is particularly popular for classification tasks. The 'naive' assumption of independence among features simplifies computation and training. Despite its simplicity, Naive Bayes often performs well, especially in text classification and spam filtering. The algorithm calculates the probability of each class given a set of features and predicts the class with the highest probability. Naive Bayes is known for its efficiency, scalability, and ease of implementation.
"""

from sklearn.naive_bayes import GaussianNB
naive_bayes = GaussianNB()

# Modeli eğitin
naive_bayes.fit(X_train_scaled, y_train)

# Test veri seti üzerinde modeli değerlendirin
y_pred_nb = naive_bayes.predict(X_test_scaled)
print("Naive Bayes - Sınıflandırma Raporu:\n", classification_report(y_test, y_pred_nb))

"""# Logistic Regression:
**Logistic Regression** is a versatile machine learning algorithm used for binary and multi-class classification. Despite its name, it is primarily employed for classification tasks. Logistic Regression models the probability that an instance belongs to a particular class using the logistic function. It operates by finding the best-fitting line (or hyperplane in higher dimensions) that separates the classes in the feature space. Logistic Regression is interpretable, computationally efficient, and serves as a fundamental algorithm in the field of machine learning. It is widely utilized in various domains for its simplicity and effectiveness.
"""

from sklearn.linear_model import LogisticRegression

logistic_regression = LogisticRegression()

param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
                 'penalty': ['l1', 'l2']}

grid_search_lr = GridSearchCV(logistic_regression, param_grid_lr, cv=5, scoring='accuracy')
grid_search_lr.fit(X_train_scaled, y_train)

print("Logistic Regression - En iyi parametreler:", grid_search_lr.best_params_)
print("Logistic Regression - En iyi skor:", grid_search_lr.best_score_)

y_pred_lr = grid_search_lr.predict(X_test_scaled)
print("Logistic Regression - Sınıflandırma Raporu:\n", classification_report(y_test, y_pred_lr))

"""# Support Vector Machine (SVM):
**Support Vector Machine** is a powerful machine learning algorithm used for classification and regression tasks. It excels in finding a hyperplane that best separates different classes in the feature space. SVM aims to maximize the margin between classes, making it robust to outliers. It is effective in high-dimensional spaces and can capture complex relationships through various kernel functions. SVM is widely employed for tasks like image classification, handwriting recognition, and bioinformatics. Its versatility and ability to handle non-linear data make it a popular choice in diverse applications.
"""

from sklearn.svm import SVC

svm = SVC()

param_grid_svm = {'C': [0.1, 1, 10],
                  'kernel': ['linear', 'rbf', 'poly'],
                  'gamma': ['scale', 'auto']}

grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')
grid_search_svm.fit(X_train_scaled, y_train)

print("SVM - En iyi parametreler:", grid_search_svm.best_params_)
print("SVM - En iyi skor:", grid_search_svm.best_score_)

y_pred_svm = grid_search_svm.predict(X_test_scaled)
print("SVM - Sınıflandırma Raporu:\n", classification_report(y_test, y_pred_svm))

"""*In conclusion, Support Vector Machine (SVM) exhibited superior performance among the evaluated algorithms. Its ability to find an optimal hyperplane for classification tasks, robustness to outliers, and effectiveness in high-dimensional spaces contributed to its outstanding results. SVM stands out as a reliable choice for this particular problem, showcasing its versatility and capability in achieving high predictive accuracy.*"""

